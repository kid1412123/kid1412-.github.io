---
title: '(CodeCrafter)大语言模型是不是发展到头了？'
category: '/小书匠/收集/知乎问答/CodeCrafter/fae26c31a6c41d7a1c8a2d20bbb59367'
slug: 'https://www.zhihu.com/question/1947700097883313342/answer/1960386006768424732'
createDate: '2025-10-11 16:47:26'
grammar_mathjax: false
grammar_footnote: false
grammar_ins: false
emoji: 'C'
tags: '人工智能语言模型,大语言模型,人工智能发展'

---


[toc]


# 问题

提问者：**<a href="https://www.zhihu.com/people/hu-die-meng-83-70">momo</a>**
提问时间: 2025-9-6 16:38:9

基于transformer的大语言模型是不是发展到头了？transformer模型使用矩阵的乘法构建语句中不同token之间的注意力机制，如果想利用矩阵来得到更先进有效的模型，我想没有比单单使用乘法构建语句内部注意力更简单有效的纠缠办法了（一些基于transformer的改进如将逐个相乘改为跳行相乘等视同与transforer同代产物）。未来新架构的大语言模型是不是需要突破现有运算逻辑，或者在构建激活函数等其他方面花费更多心思，来进行突破？

# 回答

回答者： **<a href="https://www.zhihu.com/people/lan-lan-wan">CodeCrafter</a>**
回答时间: 2025-10-11 16:47:26
点赞总数: 1158
评论总数: 80
收藏总数: 2392
喜欢总数：48

谢邀。

我算是国内最早那批搞算法的，从PC时代的推荐系统，到移动互联网时代的计算广告、图像识别，再到现在一头扎进大模型里带团队，搞了十几年了。不敢说自己是什么顶级专家，但至少这几波技术浪潮的浪尖和浪底，我都亲身感受过。

 **所以，关于你的问题，我先把结论拍在这儿，说死了：** 

 **大语言模型，特别是基于Transformer的这一套，离“到头了”还差得远，可以说十万八千里。非但没到头，我们现在看到的，可能只是冰山刚露出水面的那一角。** 

但是！你如果问我现在是不是个普通人无脑冲进来就能发财的好时机，那我得劝你冷静一下。这水现在是又深又浑，外面看着波澜壮阔全是机会，里边儿已经是巨头林立、暗流涌动，小舢板进来，一个浪头就可能给你拍碎了。

利益相关，我肯定不希望这个赛道卷得大家都没饭吃。但作为过来人，有些实话，不吐不快。这篇文章会很长，全是干货，没那么多花里胡哨的辞藻，你耐心看，肯定有收获。

###  **一、为什么说Transformer这套“简单粗暴”的玩法，远没有到头？** 

你问题的核心，其实是觉得Transformer的内在逻辑——矩阵乘法搞注意力——有点太“简单”了，似乎不像是个能撑起一场技术革命的终极答案。

这个想法，太正常了。当年我第一次看《Attention Is All You Need》那篇论文的时候，也有类似的感觉。就这么个东西？没有RNN的时序依赖，也没有CNN的局部归纳偏置，纯靠一个“自注意力”机制，用矩阵运算“大力出奇迹”？感觉有点……“没那么聪明”。

但干了这么多年工程，我早就明白一个道理： **在工程世界里，能用钱和规模解决的问题，往往是最高效、最可靠的问题。而Transformer架构，就是“暴力美学”的极致体现，它最大的优点，恰恰就是你认为的那个“缺点”——简单。** 

 **1. 简单，意味着可以被极致地并行和优化。** 

你想想，Transformer的核心是什么？说白了就是一堆矩阵乘法（Attention里的Q、K、V相乘）和几个全连接层（FFN）。这两种运算，是现代GPU（图形处理器）最最擅长干的事。GPU这玩意儿，天生就是为大规模并行计算设计的，几千个核心同时开干，算矩阵乘法简直是砍瓜切菜。

所以，Transformer的结构和现代计算硬件形成了完美的“共振”。你可以把模型做得非常深（几百层），非常宽（几万的隐藏层维度），然后用成千上万张GPU去堆，只要你肯砸钱，模型的性能（通常）就能肉眼可见地往上涨。这种“可预测的规模化收益”（Scaling Law），是之前所有AI架构都不具备的。

这就像什么呢？就像你发现了一种发动机，结构特简单，就是烧油然后爆燃做功，但只要你把气缸做得更大、数量搞得更多，发动机的马力就能蹭蹭往上涨。虽然看起来不如斯特林发动机或者电动机那么“优雅”，但它力大砖飞，而且整个工业体系（炼油、机械加工）都是为它服务的。Transformer就是AI领域的“内燃机”，而CUDA、TensorRT这些就是配套的“工业体系”。

你提到的那些“跳行相乘”之类的改进，比如各种稀疏注意力、线性注意力，本质上都是在这个“内燃机”的框架内做优化，比如怎么更省油（节省计算量/显存），怎么提高转速（加快推理速度），但核心的“爆燃做功”模式没变。

 **2. 瓶颈根本不在于矩阵乘法，而在于更宏观的层面。** 

现在限制大模型发展的，根本不是Transformer这个结构本身不够精巧。而是三个更要命的“天花板”：

 **第一堵墙：高质量数据的枯竭。** 

你可能觉得互联网上的数据取之不尽，其实不然。GPT-4级别的模型，据说已经把互联网上能找到的、有价值的公开文本数据（英文为主）“清洗”和“吸收”得差不多了。Common Crawl这些数据集，里面充斥着大量垃圾信息、重复内容和偏见。为了喂饱下一代更大的模型，数据从哪儿来？  
现在大家都在想办法：搞高质量的合成数据（让模型自己生成数据再学习，但有“近亲繁殖”导致模型能力退化的风险）；去挖掘那些没有被充分利用的数据，比如视频、音频、科学文献、高质量的私有数据（但有版权和隐私问题）。数据，已经从一个工程问题，变成了整个行业的战略资源问题。没有新“燃料”，再牛逼的发动机也得趴窝。

 **第二堵墙：算力的物理和经济极限。** 

Scaling Law虽然有效，但它不是免费的。训练一个GPT-4级别的模型，电费可能就高达几千万美元，需要一个小型发电站来供电。整个训练集群的硬件成本更是数十亿美金。下一代模型只会更夸张。这个游戏的入场券，已经贵到只有少数几个国家和巨头公司才玩得起。而且，芯片制程的摩尔定律正在放缓，功耗墙的问题越来越突出。我们不可能无限地堆算力。怎么在有限的算力预算下，达到更好的模型效果？这才是现在所有大厂的核心议题。所以，各种模型压缩、量化、分布式训练优化技术才这么火。

 **第三堵墙：对齐（Alignment）和可控性的挑战。** 

这可能是最难的一堵墙。模型能力越来越强，就像你养了一头越来越聪明的猛兽，但怎么保证它听你的话，而且是“打心底里”理解并遵循你的意图，而不是“假装听话”？  
这就是“对齐”问题。现在的RLHF（基于人类反馈的强化学习）就像是手里拿着一根胡萝卜一根大棒，在驯服这头猛兽。但效果并不完美，模型还是会“说谎”（Hallucination）、有偏见、被轻易地“越狱”（Jailbreak）。

我举个我工作里遇到的真实例子。我们之前想训练一个代码助手，帮程序员自动修复代码里的bug。我们用海量的“有bug的代码 -> 修复后的代码”数据对去微调一个大模型。结果呢？模型学会了修复很多简单的bug，表现不错。但有一次，在一个涉及数据库查询的复杂bug上，模型给出的“修复方案”表面上看逻辑通顺，代码也能跑，但实际上引入了一个更隐蔽的SQL注入漏洞！

为什么？因为模型根本不“理解”什么是“安全”，它只是从数据里学到了一个“看起来最像正确答案”的模式。它优化的是“代码表面上的正确性”，而不是“代码深层次的安全性”。这就是典型的对齐失败。让模型真正理解人类复杂的、多层次的、甚至是相互矛盾的价值观和意图，这已经有点哲学问题了，远不是改改激活函数、换个注意力机制就能解决的。

所以你看，我们现在面临的挑战，是数据、算力和对齐这三座大山。相比之下，Transformer本身那个矩阵乘法是不是最优的，反而成了次要矛盾。在攀登这三座大山的过程中，Transformer这个“登山杖”，虽然不完美，但够用，而且是目前最顺手的一根。

###  **二、未来的突破口在哪？是新架构，还是别的什么？** 

你猜的没错，大家肯定不会吊死在Transformer这一棵树上。学术界和工业界的研究员们，每天都在琢磨新东西。

 **1. 架构上的“挑战者们”** 

确实有一些非Transformer的架构正在崭露头角，它们试图从根本上解决Transformer的一些固有缺陷，比如二次方复杂度的计算成本问题。

 **状态空间模型（SSM），比如Mamba。**  这是一条非常有希望的路线。它借鉴了经典控制理论里的一些思想，用一种类似RNN的循环方式来处理序列信息，但又通过巧妙的设计做到了可以像Transformer一样并行训练。它的优势在于，处理长序列的计算复杂度是线性的（O(N)），而Transformer是平方的（O(N^2)）。这意味着在处理超长文档、高分辨率图像甚至DNA序列时，Mamba这类模型有天然的优势。现在很多公司都在研究和实验，但它能否在通用语言能力上全面超越优化到极致的Transformer，还需要更大规模的验证。生态、工具链都还不成熟，是个“潜力股”。

 **RWKV等结合了RNN和Transformer思想的模型。**  这类模型试图取两者之长，既有RNN的低推理成本，又有Transformer的并行训练能力。

但是，请注意，这些新架构目前都还处于“挑战者”的姿态。Transformer的“护城河”太深了，不仅仅是模型本身，还包括围绕它建立的一整套生态系统：从底层的CUDA/cuDNN算子库，到上层的Pytorch/Tensorflow框架，再到Hugging Face这样的模型和工具社区。想推翻这个“王朝”，需要一个在性能、效率、效果上都实现数量级优势的“革命性”产品，而不仅仅是“渐进式”的改进。

 **2. 真正的突破可能来自“架构之外”** 

在我看来，未来几年，比颠覆Transformer架构更可能发生、也更具影响力的突破，可能来自以下几个方面：

 **模型与“世界”的交互：Agent和工具使用。** 

现在的LLM，本质上还是个“缸中之脑”，它只能处理你喂给它的文本。但如果给它“手”和“脚”呢？让它能够调用计算器、搜索引擎、数据库API，甚至控制机器人去操作物理世界，会怎么样？  
这就是现在大火的AI Agent的概念。LLM作为“大脑”负责思考、推理和规划，然后生成指令去调用各种“工具”（Tools）来执行。比如，你问它“明天上海天气怎么样，帮我订一张去北京最早的高铁票”，一个Agent化的LLM会把这个复杂任务分解：1. 调用天气API查询上海天气；2. 调用12306 API查询最早的高铁；3. 调用支付API完成订票。  
这个方向的想象空间，比单纯提升模型的文本生成能力要大得多。这不再是模型架构的问题，而是系统工程、软件架构和人机交互的问题。 **未来，最牛逼的AI公司，可能不是模型做得最大的，而是Agent系统做得最聪明的。** 

如果你想真正理解agent技术是怎么落地的，那 **肯定是要去关注业内最顶尖的公司的实际落地场景** 。

字节就是一个很好的关注对象，因为它的版图足够大，所以它的agent手册就可以覆盖agent从底层技术（ **大模型、工具调用、API 集成、架构设计** ）到各种泛业务场景（ **办公、电商、内容创作、教育** ）的全链路案例。

[字节内部Agent实践手册.pdf](https://mp.weixin.qq.com/s/EPG7UNdBvlQmlv3TtSm80A)

这个手册里面字节的agent案例就可以有一套完整的框架和思路，从而收获一个比较全景的视角。比如 **飞书里的智能办公**  agent怎么自动排会生成会议纪要； **抖音电商的agent怎么实现库存监控、智能客服、定价优化** ；内容创作的agent怎么辅助创作者构思脚本和选素材； **教育场景的agent 怎么给学生定制学习计划和实时答疑** 。

 **多模态的深度融合。** 

人类认识世界，是靠眼睛、耳朵、触觉等多种感官协同的。现在的模型，虽然也能处理图像和声音，但大多还是“各自为战”或者“浅层融合”。未来的模型，一定是能像人一样，听着你的话，看着你的表情，同时阅读屏幕上的文档，然后做出反应。这种深度的多模态融合，可能会催生出新的模型架构，也可能会在现有架构上，通过数据和训练方式的革新来实现。比如Google的Project Astra演示的那样，AI能实时地理解摄像头看到的世界，并与你进行流畅的对话。这背后的技术挑战是巨大的。

 **“后训练”阶段的革命：新的对齐和微调技术。** 

如前所述，对齐是个大难题。RLHF虽然开创了先河，但它的过程昂贵、低效，且非常依赖于人类标注员的水平。未来一定会出现更高效、更自动化的对齐方法。比如现在很火的DPO（直接偏好优化），就试图直接从偏好数据中学习，而不需要一个额外的奖励模型。可能还会有基于模型“自我思辨”、“自我纠错”的对齐方法。谁能在“驯兽”这件事上取得突破，谁就能用同样的“猛兽”（基础模型），做出更安全、更好用、更可靠的产品。

 **对“智能”的更深理解。** 说到底，我们现在对“智能”和“理解”的机制，还知之甚少。大模型展现出的能力，很多时候我们只能“知其然，而不知其所以然”。这里面还有巨大的科学问题有待探索。比如，模型内部到底是如何形成“世界模型”的？“涌现”能力的本质是什么？这些基础理论的突破，可能会反过来指导我们设计出全新的、效率高得多的模型架构。但这可能需要更长的时间，甚至需要神经科学、认知科学等领域的交叉合作。

所以，你看，整个大模型领域，就像一个巨大的工地，到处都是正在施工的脚手架。Transformer只是其中最核心的那栋楼的地基和框架，虽然牢固，但整栋大楼的装修、水电、智能化系统，都还有大量的工作要做。甚至旁边还有好几块地，正在打着新地基，准备盖完全不一样的楼。

你说，这是不是发展到头了？

###  **三、给后辈的一些实在建议：想入行，现在该怎么学？** 

说了这么多，肯定有些同学听得热血沸沸，也有些同学听得心灰意冷。最后，给真心想在这波浪潮里“上岸”的同学，尤其是还没入行的，提几句掏心窝子的话。

别信外面那些培训机构吹的什么“三个月速成AI大模型专家”、“人人都能靠GPT月入十万”，那都是骗小白的。这行的门槛，现在是前所未有的高，对综合能力的要求非常苛刻。但如果你真有决心，路线图还是很清晰的。

 **第一步：打好“地基”，这玩意儿没法速成。** 

这个地基，就是数学和计算机科学基础。我面过太多简历光鲜，但基础一塌糊涂的候选人，这种人走不远。

 **数学：** 

1.   **线性代数：**  必须滚瓜烂熟。向量、矩阵、张量、特征值……这些就是大模型的“砖瓦”。你连矩阵乘法怎么算的都说不清楚，怎么可能理解Transformer？推荐看Gilbert Strang的《线性代nals》公开课和书。
2.   **微积分：**  梯度、偏导数、链式法则……这些是模型训练（梯度下降）的灵魂。不懂这个，你看优化器相关的代码和论文就是天书。
3.   **概率论与统计：**  从语言模型的本质（预测下一个词的概率）到各种评估指标，无处不在。

 **强烈推荐！！！！** 如果你想系统补上深度学习、数据科学所需的数学短板，又怕传统教材枯燥难啃，这套可视化数学丛书《鸢尾花书》非常值得一看。

[看完零基础数学图册《鸢尾花书》，机器学习数学不再难！（附PDF下载）](https://mp.weixin.qq.com/s/tt5HqOtME7Gt2CnNnJ0ngA)

我觉得它最大的特点就是 **极其可视化** 。它不像传统教材那样堆砌公式，而是用了海量的图解，把抽象的数学概念画给你看，帮你建立非常直观的理解。

它不是一本单纯的数学书，定位是“一条龙”服务： **从Python编程入手 -> 串联起机器学习必需的数学知识 -> 最后落地到机器学习理论** 。整个体系是连贯的，专门为转行或者数学基础薄弱的同学设计。

 **计算机科学：** 

1.   **编程能力：**  Python是必须的，而且要精通。不是说会写`if/else`和`for`循环就行，你要懂面向对象、数据结构与算法、多线程/多进程。因为现在搞大模型，很多时候是在做性能优化，写工程代码，而不是调个API就完事。
2.   **操作系统与计算机体系结构：**  你至少得明白CPU和GPU的区别，知道什么是内存、什么是显存，为什么数据在两者之间拷贝会慢。这样你才能理解为什么有些操作在GPU上快，为什么显存会成为瓶颈。

我个人觉得，不管是不是科班出身，每一个程序员都应该花时间了解和学习计算机科学相关的基础知识， **因为所有关于如何编程的底层逻辑和原理都在那里了。** 

这里有4本手册， **全网累积下载100w次** ，几乎程序员人手一套，包含 **数据结构与算法、操作系统、计算机组成原理、计算机网络等** 硬核基础知识，图文+实战案例，平时开发+搞定面试，帮你快速建立对计算机科学的大局观，夯实计算机基本功，瞬间起飞～

[全网累计下载100w+次，瞬间让你起飞的计算机基础知识](https://mp.weixin.qq.com/s/igbCF4F-K0DJDxVrMJ9ViQ)

[程序员必读的四本硬核底层的计算机书籍（附PDF下载）](https://mp.weixin.qq.com/s/NvU0rBlklp0JT4e_JEFwgg)

 **第二步：系统学习“经典”的机器学习和深度学习。** 

别一步就跳到大模型，你会摔死的。先从经典理论学起，建立起完整的知识体系。

1.   **入门课程：**  吴恩达的Machine Learning和Deep Learning专项课程，虽然有点老，但作为入门，建立知识框架，再好不过。
2.   **进阶课程：**  斯坦福的CS231n（计算机视觉，但对理解CNN和深度学习原理非常有帮助）、CS224n（自然语言处理与深度学习，这是LLM的直系祖宗）。
3.   **必读书籍：**  《深度学习》（花书），这本是圣经，虽然难啃，但值得反复读。

 **第三步：聚焦大模型，理论与实践结合。** 

当你有了前面的基础，就可以开始主攻大模型了。

 **必读论文：** 

 **《Attention Is All You Need》：**  逐字逐句地读，最好自己跟着网上的教程，用PyTorch把原始的Transformer复现一遍。这是“祖师爷”，必须拜。

然后是BERT、GPT系列（GPT-1, 2, 3）、T5、Llama系列的论文。了解整个技术演进的脉络。

 **动手实践：** 

1.   **Hugging Face全家桶：**  这是现在搞LLM的事实标准。不要只当个API调用工程师，去学习`transformers`库的源码，看看`BertForSequenceClassification`这类模型是怎么实现的。
2.   **从零实现：**  跟着Andrej Karpathy的`minGPT`或者`nanoGPT`项目，亲手用几百行代码写一个能跑的GPT。这个过程，比你看一百篇博客都有用。你会对模型训练的每一个细节都有刻骨铭心的理解。
3.   **微调（Fine-tuning）：**  找个开源模型（比如Llama 3, Qwen），找个垂直领域的数据集，亲手跑一遍微调流程。学习如何处理数据、设置超参数、评估模型效果。这是现在企业里最常见的应用方式。想要搞懂微调，可以看看[字节内部大模型微调实践手册.pdf](https://mp.weixin.qq.com/s/XdlScZH4Lj165_4RTvZLIg)（纯free，自由获取），它能让你看到字节在一线的探索，这个手册里结合了 **抖音、飞书、电商、智能客服等 50 多个真实业务场景，** 总结了 **SFT、ReFT、Adapter、LoRA** 等方法在千亿甚至万亿参数模型上的应用实践，还特别关注低资源和零资源场景下的微调策略，把微调方法、模型评估、部署监控全流程都搞清楚了。

很多人以为去大厂搞大模型算法有多难，其实难的不是技术本身，而是你根本找不到一个靠谱的带你入门的人或者资料。网上那些面经不是零碎就是过时，更别提系统性总结了。 **尤其是想进字节这种宇宙尽头大模型应用岗，光靠 Leetcode 和**   **huggingface**   **根本不够。** 

这就是我为什么花了很多时间，掘地三尺从 **知乎、牛客、V2EX、小红书、技术博客、GitHub、微信群** 几十个地方翻出来，把所有和「 **字节跳动大模型应用算法岗** 」相关的面试题搜集起来，整理成了这个手册—— **内容涵盖大模型原理、训练与微调、推理加速、数据工程、业务落地等五大核心维度，不仅是题目集合，更是思路梳理和实战指南** 。每道题都给出核心回答逻辑、代码实现关键点、面试官可能追问的陷阱，就像有个靠谱mentor带着你一点点拆解和构建自己的答题框架。

[字节大模型算法岗面试手册](https://mp.weixin.qq.com/s/WSKyGfUk6jAnctT25r8WFQ)

除了字节这份，还可以看看 **国内其他大模型公司的常问面试题合集** ，交叉着看，基本就能覆盖大部分考点了。除了这些，你还要重点准备AI系统设计题，比如“如何设计一个类似GitHub Copilot的代码补全系统？”或者“用大模型来重新设计一个网易云音乐的推荐系统，你会怎么做？”这类问题，考察的就是你的综合能力。

[国内大模型公司常问面试题，按方向分类一网打尽](https://mp.weixin.qq.com/s/PBp1jX_uBxkTAHr7cDS-tg)

 **最后，也是最重要的一点：保持耐心和热情，不要急功近利。** 

这个领域的技术迭代速度是按“周”来计算的。今天你刚学会的技术，下个月可能就过时了。唯一不变的，是你扎实的基础和你持续学习的能力。

别总想着去追最火、最新的模型，那没意义。把基础打牢，理解核心原理，然后找一个你感兴趣的方向（比如多模态、Agent、模型安全），深挖下去，做一个“T”字型人才。这样，无论浪潮怎么变，你都能稳稳地站在冲浪板上。

不知不觉码了这么多字，希望对你有所帮助。

  

原文地址：[(CodeCrafter)大语言模型是不是发展到头了？](https://www.zhihu.com/question/1947700097883313342/answer/1960386006768424732) 


